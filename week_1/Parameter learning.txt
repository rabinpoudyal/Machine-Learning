Gradient descent for minimizing arbitary function f(J)

Consider we have some function J( Î˜-not, Î˜-one)
and we want to minimize ğš¹-not and ğš¹-one in the function J

Outline for gradient descent:
Start with some ğš¹-one and ğš¹-two
Keep changing ğš¹-not and ğš¹-one till we find end up at minimum.

the figure is like a hill. 
example. imagine one is standing in the top of hill and wants to climb down as quickly as possible so what he does is roughly guesses the direction and goes down repeatedly. and what we want to do in gradient descent algorithm is the look around the datas and find the quickest way of minimizing it as soon as possible.

Gradient descent algorithm defination:

repeat until convergence {
	ğš¹j := ğš¹j - Î± âˆ‚__J(ğš¹-not,ğš¹-one) for(j=0 and j=1)
		      âˆ‚ğš¹j
}
where := is the assignment
Î± is the learning rate. in the previous e.g it is the 	steepness that the person takes to climb down the 	hill. This shows how fast he can climb down

In this equation we are simultaneously update theta-not and theta-one

Simulatneous Update:

temp0 := ğš¹-zero - Î± âˆ‚__J(ğš¹-not,ğš¹-one) 
		    âˆ‚ğš¹-zero
temp1 := ğš¹-one - Î± âˆ‚__J(ğš¹-not,ğš¹-one) 
		   âˆ‚ğš¹-one
ğš¹-zero := temp0
ğš¹-one := temp1
