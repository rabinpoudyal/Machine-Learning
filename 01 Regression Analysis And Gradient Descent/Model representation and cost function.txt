Model Representation:



Training set of datas

---------------------
x     |     y       |
1			2		|
2			5		|
3			8		|
4			9		|
5			10		|
6			55		|
---------------------

gives:

 => Learning Algorithm =>

 h(hypothesis)[f(x) => estimated price]

 A hypothesis is a function that maps x to y i.e takes the x-coordinates and calculates y.ie takes the size and calculate price of house.


Cost Function(Squared error cost function):
How to fit best linear line to our data?
Choose theta-not and theta-one such that the output function h-theta-not is close to the training examples.
That is I want to make this equation as small as possible:
					h-theta(x) - y-i
This is actually the error between the hypothesis and the actual value so we want to minimize as much as we can.
Also called sum of squares of errors.

Say we are given A trainining set of datas - size in feet^2(x) and price in $
Let our hypothesis is: 

h-theta(x) = theta-not + theta-one(x)
where,
theta-i's are the parameters of the model

So how to choose theta zero and theta one?
Lets go throught the following cases:


if theta-not = 1.5
     theta-one = 0 then
h(x) = 1.5 + 0*x
which gives a straight line horizontal meeting x-axis at theta not


if theta-not = 0
     theta-one = 0.5 then
h(x) = 0 + 0.5*x
which gives a straight line regression diagonal


if theta-not = 1
     theta-one = 0.5 then
h(x) = 1 + 0.5*x
which gives a straight line passing above the origin meting x-axis at theta-not


So here basically theta-one is a slope and theta-not gives where the line will meet at y-axis(y-intercept) in
y = mx + c


In linear regression we need to come up with the data set that best match the given situation. So how do we determine theta-not and theta-one?
Idea: choose theta-not, theta-one so that hypothesis function h-theta(x) is close to y for our training examples(x,y)


Here we want to minimize the theta-not and theta-one ie cost of house and square feet by using sum of square of errors

J(theta-not,theta-one) = 1/2m * Sigma(h-not(x-i) - y-i)^2 for sigma i=1 to m ie training examples

here h-not-x-i = theta-not + theta-one-x-i

where J(theta-not,theta-one) is the cost function we want to minimize



