Gradient descent for minimizing arbitary function f(J)

Consider we have some function J( Θ-not, Θ-one........Θ-n)
and we want to minimize 𝚹-not , 𝚹-one, ........ Θ-n in the function J

Outline for gradient descent:
Consider we have only two variable in the function : Θ-1,Θ-2
Start with some initial guess 𝚹-one and 𝚹-two
Keep changing 𝚹-not and 𝚹-one till we hopefully end up at minimum.

the figure is like a hill. 
example. imagine one is standing in the top of hill and wants to climb down as quickly as possible so what he does is roughly guesses the direction and goes down repeatedly. and what we want to do in gradient descent algorithm is the look around the datas and find the quickest way of minimizing it as soon as possible.

Gradient descent algorithm defination:

repeat until convergence {
	𝚹j := 𝚹j - α ∂__J(𝚹-not,𝚹-one) for(j=0 and j=1)
		      ∂𝚹j
}
where := is the assignment
α is the learning rate. in the previous e.g it is the 	steepness that the person takes to climb down the 	hill. This shows how fast he can climb down

In this equation we are simultaneously update theta-not and theta-one

Simulatneous Update:

temp0 := 𝚹-zero - α ∂__J(𝚹-not,𝚹-one) 
		    ∂𝚹-zero
temp1 := 𝚹-one - α ∂__J(𝚹-not,𝚹-one) 
		   ∂𝚹-one
𝚹-zero := temp0
𝚹-one := temp1
<<<<<<< HEAD

Note: In simultaneous update first update temp0 and temp1 with theta-not and theta zero. Then only in next step put temp0 and temp1. Do not update as soon as finding the values.

Example:
If theta-not = 1 and theta-one = 2 and the equation is 𝚹-j := 𝚹-j + sqrt(𝚹-0*𝚹-1)
then,
	𝚹-0 = 1 + sqrt(1*2) = 1 + sqrt(2)
	𝚹-1 = 2 + sqrt(2*1) = 2 + sqrt(2)

Note: simulataneous update same as Gaussian Iteration 


=======
>>>>>>> a18b895e8cfcc9af7a44d13171303254dec3636d
