Gradient descent for minimizing arbitary function f(J)

Consider we have some function J( Î˜-not, Î˜-one........Î˜-n)
and we want to minimize ğš¹-not , ğš¹-one, ........ Î˜-n in the function J

Outline for gradient descent:
Consider we have only two variable in the function : Î˜-1,Î˜-2
Start with some initial guess ğš¹-one and ğš¹-two
Keep changing ğš¹-not and ğš¹-one till we hopefully end up at minimum.

the figure is like a hill. 
example. imagine one is standing in the top of hill and wants to climb down as quickly as possible so what he does is roughly guesses the direction and goes down repeatedly. and what we want to do in gradient descent algorithm is the look around the datas and find the quickest way of minimizing it as soon as possible.

Gradient descent algorithm defination:

repeat until convergence {
	ğš¹j := ğš¹j - Î± âˆ‚__J(ğš¹-not,ğš¹-one) for(j=0 and j=1)
		      âˆ‚ğš¹j
}
where := is the assignment
Î± is the learning rate. in the previous e.g it is the 	steepness that the person takes to climb down the 	hill. This shows how fast he can climb down

In this equation we are simultaneously update theta-not and theta-one

Simulatneous Update:

temp0 := ğš¹-zero - Î± âˆ‚__J(ğš¹-not,ğš¹-one) 
		    âˆ‚ğš¹-zero
temp1 := ğš¹-one - Î± âˆ‚__J(ğš¹-not,ğš¹-one) 
		   âˆ‚ğš¹-one
ğš¹-zero := temp0
ğš¹-one := temp1
<<<<<<< HEAD

Note: In simultaneous update first update temp0 and temp1 with theta-not and theta zero. Then only in next step put temp0 and temp1. Do not update as soon as finding the values.

Example:
If theta-not = 1 and theta-one = 2 and the equation is ğš¹-j := ğš¹-j + sqrt(ğš¹-0*ğš¹-1)
then,
	ğš¹-0 = 1 + sqrt(1*2) = 1 + sqrt(2)
	ğš¹-1 = 2 + sqrt(2*1) = 2 + sqrt(2)

Note: simulataneous update same as Gaussian Iteration 


=======
>>>>>>> a18b895e8cfcc9af7a44d13171303254dec3636d
